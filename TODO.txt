DONE: test multiplication and addition speed double vs float on simplest examples - double still marginally faster
DONE: add branches and measure difference for above mentioned test - branches do slow down, better to have the decision made using compile time constants, branches would not be generated by compiler
DONE: optionally, override the is_float_matrix to #define returning just true, compilation hopefully optimizes branching away - done, see above
TODO: proper macOS + Apple M1 ARM64 cmake
TODO: proper x86-64 vs ARM64 bench
TODO: better branching off (matrix.h) support
TODO: disassembly? DONE for DEBUG binaries
TODO: disassembly of release/optimised binaries

FIXME: mnist on float still fails to successfully train
FIXME 3: gradients (delta_weights etc.) seem to be zero in float training mode :-(
